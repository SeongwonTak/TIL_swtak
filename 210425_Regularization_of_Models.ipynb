{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210425_Regularization of Models.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPCtdw0uSXyKZdUwuBR9K6m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongwonTak/TIL_swtak/blob/master/210425_Regularization_of_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAMqmHmLXLT9"
      },
      "source": [
        "# Regularization of Models\n",
        "모델의 정규화(Regularization)에 대해 알아보고, 특히 선형회귀에서의 정규화에 대해 자세히 알아보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8qcWhtkbr5O"
      },
      "source": [
        "[ 참고자료 / 출처 ]\n",
        "\n",
        "1. https://mangkyu.tistory.com/37\n",
        "2. 머신러닝 교과서 p102-104, p370-371\n",
        "3. https://hwiyong.tistory.com/93\n",
        "4. scikit-learn 공식 문서 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOPRKApY8ro"
      },
      "source": [
        "## 정규화의 이유\n",
        "정규화를 고려하는 이유는 과적합 문제를 피하려는 방법 중의 하나이다.\n",
        "\n",
        "과적합 문제의 경우는 많은 Feature를 가진 data set에서 가설함수가 훈련 데이터에 대해서는 비용함수가 0에 근접할 정도로 매우 예측을 잘 하겠지만, 새로운 데이터 즉, 검증용 데이터에 대해서는 일반화를 잘 하지 못할 것이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TY1ZCfOZ7Jp"
      },
      "source": [
        "### 과적합을 해결하는 방법\n",
        "\n",
        "과적합을 해결하는 방법에는 여러가지가 있다.\n",
        "1. 불필요한 Feature들을 먼저 제거해주자.\n",
        "2. 규제를 사용하여 모델의 복잡도를 조정하자.\n",
        "\n",
        "이를 조금 다른 방향으로 접근해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdLRY2OuamJX"
      },
      "source": [
        "## 계수에 가중치를 부여하는 정규화\n",
        "\n",
        "Overfitting 문제를 다음과 같이 고려하자.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYy4EtYXa6pf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xeOrq02a7E-"
      },
      "source": [
        "좌측의 그림의 경우는 y값을 잘 예측하고 있으나, 우측의 경우는 overfitting이 발생한 경우이다. 따라서 Overfitting 문제를 해결 하기 위해 고차수의 계수들을 0으로 가깝게 하려고 한다. \n",
        "\n",
        "이를 할 경우, 가설 함수는 smooth해질 것이며, 고차수 계수들에 Penalty를 부여하며 고차 함수를 저차함수로 smooth하게 만들 수 있을 것이다.\n",
        "\n",
        "이를 바탕으로 최소화 해야하는 오차함수를 다시 써보면 다음과 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x49dFAkFcXdY"
      },
      "source": [
        "$$ J(\\theta) = \\frac{1}{2m}\\sum^{m}_{i=1}(h_{\\theta}(x^{(i)})-y^{(i)})^{2} + \\lambda \\sum_{j=1}\\theta_{j}^{2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_G3LEfTfifO"
      },
      "source": [
        "## 선형회귀의 정규화의 여러 방법\n",
        "선형회귀의 경우 정규화 방법들에는 다음 것들이 있다.\n",
        "- Ridge Regressoin (릿지 회귀)\n",
        "- Least Absolute Shrnkage and Selection Operator (라쏘 회귀)\n",
        "- Elastic Net (엘라스틱 넷)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2naGSu2hnJf"
      },
      "source": [
        "### Ridge Regression\n",
        "릿지 회귀는 L2-norm을 사용한 회귀이다. 단순히, 최소 제곱 비용 함수에 가중치(계수)의 제곱합을 추가한 모델이다.\n",
        "\n",
        "$$ J(w)_{ridge} = \\sum^{n}_{i=1}(y^{(i)}-\\hat{y}^{(i)})^{2} + \\lambda ||w||^{2}_{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zaMptZggcr3"
      },
      "source": [
        "#초기화 및 정의 방법\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge(alpha = 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFyEYoS2huEM"
      },
      "source": [
        "### Lasso Regression\n",
        "라쏘 회귀의 경우에는 L1-norm을 사용한 회귀이다. 이의 경우, 규제 강도에 따라 어떤 가중치는 0이 될 수 있다. 즉, 라쏘 회귀의 경우에는 지도 학습의 특성 선택 기법으로 사용할 수 있다.\n",
        "\n",
        "$$ J(w)_{rasso} = \\sum^{n}_{i=1}(y^{(i)}-\\hat{y}^{(i)})^{2} + \\lambda ||w||_{1}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6xxOqTciAUD"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha = 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJTcc2XriAvM"
      },
      "source": [
        "### Elastic Net\n",
        "Elastic Net의 경우는 Ridge Regression과 Lasso Regression를 결합한 형태의 회귀이다.\n",
        "\n",
        "$$ J(w)_{ElasticNet} = \\sum^{n}_{i=1}(y^{(i)}-\\hat{y}^{(i)})^{2} + \\lambda_{1} ||w||^{2}_{2}+\\lambda_{2} ||w||$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzFWV6o6iDRs"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "elanet = ElasticNet(alpha = 1.0, l1_ratio = 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDrFFxVq9EeB"
      },
      "source": [
        "여기서 alpha값과 l1 ratio 값은 위와 조금 다른데,\n",
        "$$ \\alpha = \\lambda_{1}+\\lambda_{2}$$\n",
        "$$ L_{1}ratio = \\frac{\\lambda{2}}{\\alpha} $$\n",
        "\n",
        "로 값을 계산하게 된다."
      ]
    }
  ]
}