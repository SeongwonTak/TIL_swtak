{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd15071",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b5c9ea",
   "metadata": {},
   "source": [
    "Link : https://wikidocs.net/21694"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670acdf",
   "metadata": {},
   "source": [
    "'를 어떻게 처리하나를 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823121e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bea7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Don't be fooled by the dark sounding name, \\\n",
    "Mr. Jone's Orphanage is as cheery as cheery goes for a pstry shop.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a83f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pstry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cf64d",
   "metadata": {},
   "source": [
    "여기서는 Don't가 Do, n't  /   Jone's는  Jone'   s로 분류된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515a43f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pstry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4f217",
   "metadata": {},
   "source": [
    "여기서는 Don ' t 3개로 분리된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d986a",
   "metadata": {},
   "source": [
    "## 표준 토큰화 : Penn TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d92d18",
   "metadata": {},
   "source": [
    "* 하이픈으로 구성된 단어는 하나로 유지,\n",
    "* don't 같은 단어는 do와 n't로 분리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4ac03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "sentence = \"Starting a home-based restaurant may be an ideal. \\\n",
    "it doesn't have a food chain or restaurant of their own.\"\n",
    "\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de0a7c",
   "metadata": {},
   "source": [
    "## 문장과 문장을 나누는 sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848526d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. \\\n",
    "Finally, the barber went up a mountain and almost to the edge of a cliff. \\\n",
    "He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135a7cf",
   "metadata": {},
   "source": [
    "# Lemmatization (표제어 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d1b7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5a8c6",
   "metadata": {},
   "source": [
    "단어를, 토큰화 한 후, 표제어를 통해 품사를 어느정도 통일화 시킬 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e75c71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatizer.lemmatize\n",
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d930d0f",
   "metadata": {},
   "source": [
    "다음과 같이, 품사를 명시하면 Lemmatization이 이루어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05940786",
   "metadata": {},
   "source": [
    "문장에서는 토큰화 한 이후, 표제어 추출로 진행될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9ee2b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'Brown', 'fox', 'jumpes', 'over', 'the', 'lazying', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"The quick Brown fox jumpes over the lazying dogs.\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print([lemmatizer.lemmatize(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51f513",
   "metadata": {},
   "source": [
    "# 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c81da0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n",
      "['the', 'quick', 'brown', 'fox', 'jump', 'ov', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print([porter_stemmer.stem(word) for word in tokens])\n",
    "print([lancaster_stemmer.stem(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd262d2f",
   "metadata": {},
   "source": [
    "# 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b59a227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ed82454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# 불용어 출력\n",
    "stop_word_list = stopwords.words('english')\n",
    "print(stop_word_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685570e8",
   "metadata": {},
   "source": [
    "## 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84caed",
   "metadata": {},
   "source": [
    "불용어 제거는 토큰화를 사전에 진행하고 먼저 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4efe8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "example = \"Family is not an important thing. It's everything you have.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "word_tokens = tokenizer.tokenize(example)\n",
    "result = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        result.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a79725fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing.', 'It', \"'s\", 'everything', 'you', 'have', '.']\n",
      "['Family', 'important', 'thing.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002422fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
