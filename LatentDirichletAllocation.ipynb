{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LatentDirichletAllocation.ipynb",
      "provenance": [],
      "mount_file_id": "1iJvkzinoI7tMLgvjoly1YBUXiydwLNQD",
      "authorship_tag": "ABX9TyOLVomT5/5JxRPLA7f7Q+lq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongwonTak/TIL_swtak/blob/master/LatentDirichletAllocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4JKYVUaklJO"
      },
      "source": [
        "# Latent Dirichlet Allocation\n",
        "잠재 디리클레 할당(Latent Dirichlet Allocation)에 대해 \n",
        "\n",
        "(참고자료)\n",
        "- https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/\n",
        "- https://wikidocs.net/30708\n",
        "- 머신러닝 교과서 p309~p314"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGDpCyu6sGLH"
      },
      "source": [
        "## 개요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4aJtowEs8pl"
      },
      "source": [
        "토픽 모델링(Topic Modeling)이란 레이블이 없는 '텍스트'에 토픽을 할당하는 분야이다.\n",
        "예를 들어, 뉴스 기사를 보고 분야를 할당하는 문제를 고려할 수 있다.(이는 clustering)\n",
        "\n",
        "LDA, 잠재 디리클레 할당이란 기본적으로, 여러 문서에 걸져 자주 등장하는 단어의 그룹을 찾는 확률적 생성 모델이다. 즉 토픽을 자주 등장하는 단어들로 나타낼 수 있다는 것에서 출발한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtEoxDBnzxpS"
      },
      "source": [
        "### BoW 행렬을 통한 LDA\n",
        "LDA에서는 입력으로 받은 BoW 행렬을 분해한다.\n",
        "- 문서-토픽 행렬\n",
        "- 단어-토픽 행렬\n",
        "\n",
        "이 두 행렬을 곱하여 가능한 작은 오차로 입력 행렬을 재구성할 수 있도록 한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X184pm-juAt2"
      },
      "source": [
        "## Scikit-Learn에서의 LDA\n",
        "\n",
        "우선 실제 구현 예시부터 확인하려고 한다.\n",
        "\n",
        "영화 리뷰 데이터 셋을 분해하여, 여러개의 토픽으로 분류하고자 한다.\n",
        "이를 위해서는 데이터셋을 불러와야 할 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etw7Cie8khaG"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/movie_review.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "m9h_K37Rzo1R",
        "outputId": "ae8b041c-4b5d-4903-9705-5e852aa98645"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HerHLtBG0QW2"
      },
      "source": [
        "이제 BoW 행렬을 만들어보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8A43XGw0Tdc"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count = CountVectorizer(stop_words='english',\n",
        "                        max_df = 0.1,\n",
        "                        max_features = 5000)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWwcZYad0hok"
      },
      "source": [
        "max_df 지정을 통해 너무 자주 등장하는 단어를 제외하였다. (너무 자주 등장하면 토픽 카테고리와 연관성이 적을 것이기에)\n",
        "\n",
        "또한 자주 등장하는 단어를 제한하였다. 이는 차원을 제한하여 오버피팅 방지 및 LDA 추론 성능을 향상시킨 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95fE0k210wAF"
      },
      "source": [
        "X = count.fit_transform(df['review'].values)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeAOvNa007AV"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components = 10,\n",
        "                                random_state = 999,\n",
        "                                learning_method='batch')\n",
        "X_topics = lda.fit_transform(X)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMgqnK-D1Hru"
      },
      "source": [
        "- 10개의 토픽에 대해 5000개의 단어를 분류할 것이다.\n",
        "- batch로 설정하여, lda가 한 번 반복할때 모든 데이터를 사용하여 학습된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grghXFqoAMup",
        "outputId": "89b661e1-76d9-4bdc-d911-5852b57bebbd"
      },
      "source": [
        "lda.components_.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKhpgPVFBGWA",
        "outputId": "29ca3b06-1292-4d23-e051-de871d0f533d"
      },
      "source": [
        "n_topic_words = 5\n",
        "feature_names = count.get_feature_names()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "  print('토픽 %d:' % (topic_idx + 1))\n",
        "  print(\" \".join([feature_names[i] for i in topic.argsort()\\\n",
        "                  [:-n_topic_words -1:-1]]))\n",
        "                 "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "토픽 1:\n",
            "music audience feel cinema beautiful\n",
            "토픽 2:\n",
            "family kids mother father girl\n",
            "토픽 3:\n",
            "action game fight fi sci\n",
            "토픽 4:\n",
            "book version read original novel\n",
            "토픽 5:\n",
            "comedy worst script awful minutes\n",
            "토픽 6:\n",
            "role performance plays john comedy\n",
            "토픽 7:\n",
            "war american men women woman\n",
            "토픽 8:\n",
            "series episode tv dvd episodes\n",
            "토픽 9:\n",
            "horror guy dead house killer\n",
            "토픽 10:\n",
            "worst money waste minutes wasn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2ZPxW0LMc7h"
      },
      "source": [
        "분류가 잘 되어있나 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPXGdndLNafg",
        "outputId": "04ef8d00-7034-4c87-e208-f3113db7663b"
      },
      "source": [
        "trash_movie = X_topics[:, 8].argsort()[::-1]\n",
        "for iter_idx, movie_idx in enumerate(trash_movie[:5]):\n",
        "  print('horror movie #%d' % (iter_idx+1))\n",
        "  print(df['review'][movie_idx][:200], ',,,')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "horror movie #1\n",
            "Why you ask does this man claim to have the truth behind the existence of the almighty? Well its deductive logic my friends, you see I know God exists because Satan does, how else would my poor eyes h ,,,\n",
            "horror movie #2\n",
            "Wow. Just wow. Never before have i seen a horror movie in which it seemed like a bad self insert fanfic that somebody wrote one day in 20 minutes. And then i happened to come upon \"Lady Frankenstein\". ,,,\n",
            "horror movie #3\n",
            "***SPOILERS*** ***SPOILERS*** Some bunch of Afrikkaner-Hillbilly types are out in the desert looking for Diamonds when they find a hard mound in the middle of a sandy desert area. Spoilers: The dumbes ,,,\n",
            "horror movie #4\n",
            "That is what this movie is. Good God the special effects suck in this movie. It is difficult for anything to suck more than this movie's plot, but the special effects manage to pull it off. Let me try ,,,\n",
            "horror movie #5\n",
            "This straight to video cheap flick is based on a true story. I don't doubt it. Doesn't mean it's particularly interesting (unless you are one of the main characters who actually lived though this expe ,,,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nIYJO7aMg7o",
        "outputId": "3fdac295-33a1-4492-a949-3bc4d9327f07"
      },
      "source": [
        "trash_movie = X_topics[:, 9].argsort()[::-1]\n",
        "for iter_idx, movie_idx in enumerate(trash_movie[:5]):\n",
        "  print('trash movie #%d' % (iter_idx+1))\n",
        "  print(df['review'][movie_idx][:200], ',,,')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trash movie #1\n",
            "This movie is the biggest waste of nine dollars that I've spent in a very, very long time. If you knew how often I went to the movies you'd probably say, that's hard to imagine, but never-the-less, it ,,,\n",
            "trash movie #2\n",
            "I rented this movie from my local library and thought it might be good considering I like this type of movie and considering who was in it but boy was I wrong. The acting stunk, the fight scenes were  ,,,\n",
            "trash movie #3\n",
            "This movie is stupid and i hate it!!! i turned it off before it reached half i hate this movie. Amitabh sucks in this movie i wanna throw eggs at the person who directed this movie. This movie is stup ,,,\n",
            "trash movie #4\n",
            "IT IS A PIECE OF CRAP! not funny at all. during the whole movie nothing ever happens. i almost fell asleep, which in my case happens only if a movie is rally bad. (that is why it didn't get 1 (awful)  ,,,\n",
            "trash movie #5\n",
            "I sat down to watch this movie with my friends with very low expectations. My expectations were no where near low enough. I honestly could not tell what genre this movie was from watching it, and if i ,,,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUrRGPVqNMNp",
        "outputId": "38168ef5-ab69-4129-a313-5bf45bf07e7e"
      },
      "source": [
        "trash_movie2 = X_topics[:, 4].argsort()[::-1]\n",
        "for iter_idx, movie_idx in enumerate(trash_movie[:5]):\n",
        "  print('trash movie2 #%d' % (iter_idx+1))\n",
        "  print(df['review'][movie_idx][:200], ',,,')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trash movie2 #1\n",
            "Do all spoof films require pure stupidity and a lack of ANY sort of intelligence whatsoever to the humour? Is there even just a single genuinely FUNNY parody film anymore? All I see are zero-quality f ,,,\n",
            "trash movie2 #2\n",
            "I haven't actually finished the film. You may say that in this case I have no right to review it, especially so negatively. But I do, only because I stopped it on account of I couldn't watch anymore.. ,,,\n",
            "trash movie2 #3\n",
            "I am shocked to see that this movie has been given more than two stars by some people. They must either be kidding or be totally blind for the art of acting, directing and other flaws of the movie.<br ,,,\n",
            "trash movie2 #4\n",
            "This movie really shows its age. The print I saw was terrible due to age, but it is possible that there are better prints out there. However, this was not the major problem with the movie. The problem ,,,\n",
            "trash movie2 #5\n",
            "This movie really shows its age. The print I saw was terrible due to age, but it is possible that there are better prints out there. However, this was not the major problem with the movie. The problem ,,,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IFEGrANNVfv"
      },
      "source": [
        "5번, 10번이 아쉽게 중복이되어버린 점이 있으나, 전반적으로 분류가 잘 되어 있고 리뷰도 알맞게 배당되어있음을 알 수 있다. 다만 시작 데이터 특성상 어떤 영화인지는 알기는 어렵다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffvCNWZ1WyzY"
      },
      "source": [
        "## LDA 모델 해석\n",
        "\n",
        "LDA에 대한 자세한 해설은 다음과 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXCKa7fBYrAB"
      },
      "source": [
        "### LDA의 기본 가정 및 과정\n",
        "\n",
        "LDA는 문서를 만드는 과정을 역공학으로 추척하는 과정을 담은 알고리즘이다.\n",
        "그럼 먼저 문서를 만드는 과정에 대해 알아보자.\n",
        "\n",
        "- 문서 사용 토픽의 혼합을 확률 분포에 기반하여 결정한다.\n",
        "- 해당 확률 분포를 기반하여 단어를 선정한다\n",
        "\n",
        "즉,  **LDA는 토픽의 단어분포와 문서의 토픽분포의 결합으로 문서 내 단어들이 생성된다고 가정한다.**\n",
        "따라서 LDA는 다음의 과정을 바탕으로 이루어지게 된다.\n",
        "\n",
        "- 사용자가 Topic의 개수를 지정한다.\n",
        "- 모든 단어를 1개의 Topic에 할당한다.\n",
        "- 단어 w가 잘못 파악이 되었을 경우 다음 확률들을 바탕으로 재분배한다.\n",
        "  - 문서 d 중 topic t에 해당되는 단어의 비율\n",
        "  - topic t 중 word w가 차지하는 비율"
      ]
    }
  ]
}