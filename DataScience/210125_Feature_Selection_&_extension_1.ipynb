{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210125_Feature Selection & extension 1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQqdjpNc4YR69vUATY2IRT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongwonTak/TIL_swtak/blob/master/DataScience/210125_Feature_Selection_%26_extension_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIw2KuyYlHKB"
      },
      "source": [
        "#210125 Feature Selection and Extension.\r\n",
        "\r\n",
        "오늘부터는 특성 선택과 추출에 대해서 공부하고자 한다. 앞에서 접해보았던 머신러닝 기법들 전체적으로 가지고 있는 해결점에 대해 고려해보자.\r\n",
        "\r\n",
        "## 0. 차원의 저주 및 특성 선택/압축 개요\r\n",
        "만일 어떤 머신러닝 분석 문제에서 특성이 1개인 경우와, 특성이 1000개인 경우 두 가지가 있다고 보자. 특성이 1개인 경우에는 분석이 1개겠지만, 특성이 1000개인 경우에는 분석의 어려움도 있겠지만 서로간의 연관성에 의해 노이즈가 더 발생할 수 있다.\r\n",
        "\r\n",
        "따라서 우리는 **중요한 특성만 선택**하던가, 특**성을 압축하여 효과적으로 분석할 길을 고려해야 할 것**이다.\r\n",
        "\r\n",
        "대표적인 특성 선택과 추출에는 다음과 같은 방법들이 있다.\r\n",
        "\r\n",
        "- 특성 선택\r\n",
        "  * Sequential feature selection\r\n",
        "  탐욕적 알고리즘 기반으로, 초기 특성 공간을 축소한다. 관계없는 특성이나 잡음을 제거해야 한다.\r\n",
        "  전통적인 순차 특성 알고리즘은 순차 후진 선택(Sequential Backward Selection, SBS)를 취한다.\r\n",
        "\r\n",
        "- 특성 압축\r\n",
        "  * Principal Component Analysis(주성분 분석)\r\n",
        "  * Linear Discriminant Analysis(선형판별분석)\r\n",
        "  * Kernel Principal Component Analysis (커널PCA)\r\n",
        "    -> 비선형 차원 축소에 사용 가능.\r\n",
        "\r\n",
        "앞으로 이 방법들이 어떤 식으로 사용되는지, 그 원리와 예시를 좀 보려고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUnCc7rbqa2G"
      },
      "source": [
        "## 1. Sequential Backward Selection의 개요\r\n",
        "\r\n",
        "순차 후진 선택은 모든 feature가 주어진 상황에서, 부분공간으로 특성을 축소시기키기 위해 하나씩 feature을 제거해 나가는 방법이다.\r\n",
        "\r\n",
        "목표는 당연히, **모델 성능을 가장 적게 희생**하는 것이다.\r\n",
        "\r\n",
        "가장 떠오르는 쉬운 방법은 **가장 많이 성능을 깨치는 것부터 하나씩 제거**해 나가는 것이다. 실제로 이를 greedy algorithm(탐욕적 알고리즘)의 예시로 볼 수 있는데, greedy algorithm이란 최적화나 탐색 문제의 각 단계에서 부분적으로 선택할 수 있는 최적의 선택을 하는 것이다.\r\n",
        "\r\n",
        "이를 위해서는 기준 함수가 필요할 것인데, 이 기준함수는 **특성을 제거하기 전의 모델의 성능 차이**이다. 기준 함수가 최대화가 되는 특성을 하나씩 제거하다가 목표하는 특성 개수가 되면 종료한다.\r\n",
        "\r\n",
        "\r\n",
        "이런, 아쉽게도 scikit-learn에는 해당 모델이 없어서 직접 코딩을 해야 한다.\r\n",
        "\r\n",
        "참조 : https://vitalflux.com/sequential-backward-feature-selection-python-example/\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOHj2Ym_rtAy"
      },
      "source": [
        "## 2. Sequential Backward Selection code\r\n",
        "\r\n",
        "코드가 쉽지는 않다. 주석을 달면서 하나하나 이해해보자.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU2M89RMrqll"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "from itertools import combinations\r\n",
        "from sklearn.base import clone\r\n",
        "import numpy as np\r\n",
        " \r\n",
        "class SequentialBackwardSearch():\r\n",
        "\r\n",
        "    def __init__(self, estimator, k_features):\r\n",
        "        self.estimator = clone(estimator)\r\n",
        "        self.k_features = k_features\r\n",
        "         \r\n",
        "    def fit(self, X_train, X_test, y_train, y_test):  # 훈련셋/테스트셋\r\n",
        "        dim = X_train.shape[1]\r\n",
        "        self.indices_ = tuple(range(dim))\r\n",
        "        self.subsets_ = [self.indices_]\r\n",
        "        score = self._calc_score(X_train.values, X_test.values, \r\n",
        "                                 y_train.values, y_test.values, self.indices_)\r\n",
        "        self.scores_ = [score]\r\n",
        "        \r\n",
        "        # 원하는 개수가 돌 때까지 돌아야 하므로\r\n",
        "\r\n",
        "        while dim > k_features:\r\n",
        "            scores = []\r\n",
        "            subsets = []\r\n",
        "\r\n",
        "            # 존재하는 특성의 조합별로 모델을 훈련하고 그 점수를 기록한다.\r\n",
        "            for p in combinations(self.indices_, r=dim - 1):\r\n",
        "                score = self._calc_score(X_train.values, X_test.values, y_train.values, y_test.values, p)\r\n",
        "                scores.append(score)\r\n",
        "                subsets.append(p)\r\n",
        "\r\n",
        "            best_score_index = np.argmax(scores) # 최고점 위치 기록\r\n",
        "            self.scores_.append(scores[best_score_index]) # 최고점 기록\r\n",
        "            self.indices_ = subsets[best_score_index] # 최고점 특성 기록\r\n",
        "            self.subsets_.append(self.indices_) # 특성의 위치 기록\r\n",
        "            dim -= 1 # 하나 날렸으니까 차원 하나 줄여야함...\r\n",
        "     \r\n",
        "    # 데이터 셋 변환을 통해 필요 변수만 추린다.\r\n",
        "    def transform(self, X):\r\n",
        "        return X.values[:, self.indices_]\r\n",
        "     \r\n",
        "    # 최종 선택 기준으로 훈련한다.\r\n",
        "    def _calc_score(self, X_train, X_test, y_train, y_test, indices):\r\n",
        "        self.estimator.fit(X_train[:, indices], y_train.ravel())\r\n",
        "        y_pred = self.estimator.predict(X_test[:, indices])\r\n",
        "        score = accuracy_score(y_test, y_pred)\r\n",
        "        return score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omSnqrhO8n2d"
      },
      "source": [
        "코드에서는 먼저, 목표 특성 개수를 k_features로 지정한다.\r\n",
        "\r\n",
        "accuracy_score 함수로 특성 부분집합에 대한 모델 성능을 평가한다.\r\n",
        "최적 조합 정확도 점수는 self.scores_ 리스트에 모은다. 이 점수를 사용하여 나중에 결과를 평가한다.\r\n",
        "최종 선택된 특성의 열 인덱스는 self.indices_에 할당된다.\r\n",
        "\r\n",
        "이를 바탕으로, transform에서 선택된 열로만 구성된 새로운 데이터를 반환한다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF6aIUwQ-jzv"
      },
      "source": [
        "## 3. PCA의 개요\r\n",
        "\r\n",
        "다음은 주성분 분석으로 불리는 PCA(Principal Component Analysis)이다. PCA는 데이터간의 상관관계를 기반으로 특성을 잡아낸다.\r\n",
        "\r\n",
        "- 분산이 가장 큰 방향을 찾는다.\r\n",
        "- 차원이 좀 더 작거나 같은 새로운 부분공간으로 투영시킨다.\r\n",
        "- 새로운 특성축은 기존의 특성축과 직교한다.\r\n",
        "\r\n",
        "Orthogonal 하게 자른다는 점에서, 우리는 어떤 행렬을 생각하여 eigenvalue를 활용하여 행렬을 쪼개야 할 것임을 예측할 수 있다.\r\n",
        "\r\n",
        "PCA에서는 차원을 축소하기 위해 변환 행렬을 만들어야 한다.\r\n",
        "$d$차원에서 $k$차원으로 축소하기 위해, 우리는 $d \\times k$ 행렬을 만들어야 한다. 행렬을 통해 벡터를 부분공간으로 mapping 시켜야한다.\r\n",
        "\r\n",
        "몇 가지 주의사항과 관찰할 수 있는 부분을 알 수 있다.\r\n",
        "\r\n",
        "- 변환 행렬을 만든다는 점에서, PCA의 방향들은 각 데이터의 범위에 민감하게 반응할 수 밖에 없다. **scaling 과정이 선행되어야만 한다**.\r\n",
        "- 모든 주성분은 다른 주성분들과 **모두 orthogonal**해야 가장 큰 분산을 가진다. 즉 **\"직교행렬\"을 만들어야 한다.**\r\n",
        "\r\n",
        "\r\n",
        "구체적인 알고리즘과 PCA와 관련된 수학적 배경은 내일 마저 공부하려고 한다."
      ]
    }
  ]
}