{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextPreprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMK0Nus0TDr09GvlcGeSPn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongwonTak/TIL_swtak/blob/master/DataScience/TextPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UT8YX0GnyxQ"
      },
      "source": [
        "# Text Preprocessing\n",
        "텍스트 분석을 위한 텍스트 전처리 방법에 대해 알아보았다.\n",
        "참고 : 머신러닝 교과서 8장. / 데이터사이언스 스쿨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh3x5BiYrbLP"
      },
      "source": [
        "## BoW 모델\n",
        "Bag-of-Word 라고 불리는 BoW 모델이란, 텍스트를 수치로 표현하는 방법이다.\n",
        "하려는 작업은 다음과 같다.\n",
        "\n",
        "1. 전체 문서에 대한 고유한 토큰(어휘사전?)을 만든다.\n",
        "2. 특정 문서에 각 단어가 얼마나 자주 등장하는지를 헤아려 특성 벡터를 만든다.\n",
        "\n",
        "물론, 전체 문서에서 특정 어휘는 모든 단어의 일부분일거므로, 특성 벡터는 sparse할 것이다.\n",
        "\n",
        "즉, 다시 말해\n",
        "- 문서 집합\n",
        "$$\\{d_{1},d_{2},...d_{n}\\} $$이 있고, \n",
        "- 단어장이\n",
        "$${t_{1},t_{2},...,t_{m{} $$이 있다면,\n",
        "\n",
        "$x_{ij}$를 문서 i내 단어 j의 출현 빈도로 표현 가능할 것이다.\n",
        "(혹은 있다 없다로 표현할수도 있을 것이고)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kMyFSJwu09_"
      },
      "source": [
        "### CountVectorizer\n",
        "scikit-learn에 있는 하나의 클래스인 CountVectorizer에 대해 알아보자.\n",
        "이는 문서에 있는 단어를 카운트한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzowZ26snst7",
        "outputId": "4db74fcd-8b2b-469c-9972-ffcd58ead325"
      },
      "source": [
        "#CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "corpus = np.array([\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?',\n",
        "    'The last document?'])\n",
        "bag = count.fit_transform(corpus)\n",
        "\n",
        "print(count.vocabulary_)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDzzHE6avp2F",
        "outputId": "5a101208-1c26-4017-a97e-395bb20d9153"
      },
      "source": [
        "print(bag.toarray())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 1 0 0 2 1 0 1]\n",
            " [1 0 0 0 0 1 0 1 1 0]\n",
            " [0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 0 1 0 0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cWUP8qyvv8v"
      },
      "source": [
        "corpus, 말 뭉치를 보고 bag에다가 단어의 종류를 넣는다.\n",
        "count_vocabulary_ 를 통해 단어의 종류를 확인 가능하다.\n",
        "단어와 인덱스의 위치가 주어지고\n",
        "\n",
        "이는 bag.toarray()를 통해 행렬로 표현 가능하다.\n",
        "예를들어 col 0은 and이고, col 1은 document... 이렇게 표현 가능하다.,.\n",
        "\n",
        "여기서 두번째 문서에는 second(인덱스 6)가 2번 나옴을 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V1MuRN5wZFt"
      },
      "source": [
        "여기서, 우리는 **단어 빈도(term frequency)** 라는 것을 정의하려고 한다.\n",
        "문서 $d$에 등장한 단어 $t$의 횟수를 $tf(t, d)$ 처럼 같이 쓴다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0CH5xcAx_EU"
      },
      "source": [
        "### Stop Words\n",
        "불용어란, 모든 종류의 텍스트에 아주 흔하게 등장하는 정보가 거의 없는 문법요소 단어들이라고 보면 된다. (is, and, has, like...)\n",
        "불용어를 제거하는 방법은 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAykRag0x-VN",
        "outputId": "0191be6a-b8fb-4afe-ed8e-e30d27e05976"
      },
      "source": [
        "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'document': 0, 'first': 1, 'last': 2, 'one': 3, 'second': 4, 'third': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "408jHq6-yitl"
      },
      "source": [
        "모든 불용어를 저렇게 지정하는 것은 매우 어렵다. \n",
        "파이썬의 nltk 패키지를 통해 불용어 리스트를 받아 처리할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4rIgHUG2UB0",
        "outputId": "452b94f9-6867-4d37-dca9-9b3d112fee50"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgdG9EEc27DT",
        "outputId": "a47388d9-2e6f-48a2-8230-1489efb978bc"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        " \n",
        "stop = stopwords.words('english')\n",
        "corpus2 = np.array(['The quick brown fox jumps over the lazy dog',\n",
        "                    'My Heart will go on',\n",
        "                    'The lazy girl breaks my heart'])\n",
        "vect2 = CountVectorizer(stop_words = stop).fit(corpus2)\n",
        "vect2.vocabulary_"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'breaks': 0,\n",
              " 'brown': 1,\n",
              " 'dog': 2,\n",
              " 'fox': 3,\n",
              " 'girl': 4,\n",
              " 'go': 5,\n",
              " 'heart': 6,\n",
              " 'jumps': 7,\n",
              " 'lazy': 8,\n",
              " 'quick': 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}