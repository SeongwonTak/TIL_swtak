{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextPreprocessing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVtGJRtBOlJrvm+eTH62VQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongwonTak/TIL_swtak/blob/master/DataScience/TextPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UT8YX0GnyxQ"
      },
      "source": [
        "# Text Preprocessing\n",
        "텍스트 분석을 위한 텍스트 전처리 방법에 대해 알아보았다.\n",
        "참고 : 머신러닝 교과서 8장. / 데이터사이언스 스쿨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh3x5BiYrbLP"
      },
      "source": [
        "## BoW 모델\n",
        "Bag-of-Word 라고 불리는 BoW 모델이란, 텍스트를 수치로 표현하는 방법이다.\n",
        "하려는 작업은 다음과 같다.\n",
        "\n",
        "1. 전체 문서에 대한 고유한 토큰(어휘사전?)을 만든다.\n",
        "2. 특정 문서에 각 단어가 얼마나 자주 등장하는지를 헤아려 특성 벡터를 만든다.\n",
        "\n",
        "물론, 전체 문서에서 특정 어휘는 모든 단어의 일부분일거므로, 특성 벡터는 sparse할 것이다.\n",
        "\n",
        "즉, 다시 말해\n",
        "- 문서 집합\n",
        "$$\\{d_{1},d_{2},...d_{n}\\} $$이 있고, \n",
        "- 단어장이\n",
        "$${t_{1},t_{2},...,t_{m{} $$이 있다면,\n",
        "\n",
        "$x_{ij}$를 문서 i내 단어 j의 출현 빈도로 표현 가능할 것이다.\n",
        "(혹은 있다 없다로 표현할수도 있을 것이고)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kMyFSJwu09_"
      },
      "source": [
        "### CountVectorizer\n",
        "scikit-learn에 있는 하나의 클래스인 CountVectorizer에 대해 알아보자.\n",
        "이는 문서에 있는 단어를 카운트한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzowZ26snst7",
        "outputId": "0e117256-3b96-46cf-f708-e877dca113d1"
      },
      "source": [
        "#CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "corpus = np.array([\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?',\n",
        "    'The last document?'])\n",
        "bag = count.fit_transform(corpus)\n",
        "\n",
        "print(count.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDzzHE6avp2F",
        "outputId": "1d34d3e0-79a0-44e4-bb62-bc155a91c2b7"
      },
      "source": [
        "print(bag.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 1 0 0 2 1 0 1]\n",
            " [1 0 0 0 0 1 0 1 1 0]\n",
            " [0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 0 1 0 0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cWUP8qyvv8v"
      },
      "source": [
        "corpus, 말 뭉치를 보고 bag에다가 단어의 종류를 넣는다.\n",
        "count_vocabulary_ 를 통해 단어의 종류를 확인 가능하다.\n",
        "단어와 인덱스의 위치가 주어지고\n",
        "\n",
        "이는 bag.toarray()를 통해 행렬로 표현 가능하다.\n",
        "예를들어 col 0은 and이고, col 1은 document... 이렇게 표현 가능하다.,.\n",
        "\n",
        "여기서 두번째 문서에는 second(인덱스 6)가 2번 나옴을 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V1MuRN5wZFt"
      },
      "source": [
        "여기서, 우리는 **단어 빈도(term frequency)** 라는 것을 정의하려고 한다.\n",
        "문서 $d$에 등장한 단어 $t$의 횟수를 $tf(t, d)$ 처럼 같이 쓴다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0CH5xcAx_EU"
      },
      "source": [
        "### Stop Words\n",
        "불용어란, 모든 종류의 텍스트에 아주 흔하게 등장하는 정보가 거의 없는 문법요소 단어들이라고 보면 된다. (is, and, has, like...)\n",
        "불용어를 제거하는 방법은 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAykRag0x-VN",
        "outputId": "217f3a28-f7e2-48aa-9f20-004585dfb463"
      },
      "source": [
        "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'document': 0, 'first': 1, 'last': 2, 'one': 3, 'second': 4, 'third': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "408jHq6-yitl"
      },
      "source": [
        "모든 불용어를 저렇게 지정하는 것은 매우 어렵다. \n",
        "파이썬의 nltk 패키지를 통해 불용어 리스트를 받아 처리할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4rIgHUG2UB0",
        "outputId": "5c5b0c6d-dbfd-41ea-b6a2-d349fb065cdb"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgdG9EEc27DT",
        "outputId": "f21b26fe-5abe-46b6-e53a-211d7bdd3737"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        " \n",
        "stop = stopwords.words('english')\n",
        "corpus2 = np.array(['The quick brown fox jumps over the lazy dog',\n",
        "                    'My Heart will go on',\n",
        "                    'The lazy girl breaks my heart'])\n",
        "vect2 = CountVectorizer(stop_words = stop).fit(corpus2)\n",
        "vect2.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'breaks': 0,\n",
              " 'brown': 1,\n",
              " 'dog': 2,\n",
              " 'fox': 3,\n",
              " 'girl': 4,\n",
              " 'go': 5,\n",
              " 'heart': 6,\n",
              " 'jumps': 7,\n",
              " 'lazy': 8,\n",
              " 'quick': 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlL12qDULcFB"
      },
      "source": [
        "### N-gram\n",
        "지금까지 1단어 기준으로만 token을 부여한 것을 1-gram이라고 한다.\n",
        "N-gram은 연속된 N개의 어절을 뜻한다.\n",
        "\n",
        "예를 들어\n",
        "'The quick brown fox jumps over the lazy dog' 의 3-gram이라면\n",
        "- The quick brown\n",
        "- quick brown fox\n",
        "....\n",
        "- the lazy dog\n",
        "이 될 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n0bOR4tOhYM",
        "outputId": "1cce67f6-1c77-4086-ecc6-78ca34caf32b"
      },
      "source": [
        "ngram_vect = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
        "ngram_vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and the': 0,\n",
              " 'first document': 1,\n",
              " 'is the': 2,\n",
              " 'is this': 3,\n",
              " 'last document': 4,\n",
              " 'second document': 5,\n",
              " 'second second': 6,\n",
              " 'the first': 7,\n",
              " 'the last': 8,\n",
              " 'the second': 9,\n",
              " 'the third': 10,\n",
              " 'third one': 11,\n",
              " 'this is': 12,\n",
              " 'this the': 13}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO8IFkSfQq0s"
      },
      "source": [
        "혹은 1단어에서 N단어까지를 지정할수도 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARJLET-1QvnF",
        "outputId": "c03a572e-9976-478e-dd8f-3ece7d34c1df"
      },
      "source": [
        "ngram2_vect = CountVectorizer(ngram_range=(1, 2)).fit(corpus2)\n",
        "ngram2_vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'breaks': 0,\n",
              " 'breaks my': 1,\n",
              " 'brown': 2,\n",
              " 'brown fox': 3,\n",
              " 'dog': 4,\n",
              " 'fox': 5,\n",
              " 'fox jumps': 6,\n",
              " 'girl': 7,\n",
              " 'girl breaks': 8,\n",
              " 'go': 9,\n",
              " 'go on': 10,\n",
              " 'heart': 11,\n",
              " 'heart will': 12,\n",
              " 'jumps': 13,\n",
              " 'jumps over': 14,\n",
              " 'lazy': 15,\n",
              " 'lazy dog': 16,\n",
              " 'lazy girl': 17,\n",
              " 'my': 18,\n",
              " 'my heart': 19,\n",
              " 'on': 20,\n",
              " 'over': 21,\n",
              " 'over the': 22,\n",
              " 'quick': 23,\n",
              " 'quick brown': 24,\n",
              " 'the': 25,\n",
              " 'the lazy': 26,\n",
              " 'the quick': 27,\n",
              " 'will': 28,\n",
              " 'will go': 29}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCeKrZplQ8Mj"
      },
      "source": [
        "## TF-IDF\n",
        "자주 등장하는 단어들은 다음과 같은 특성을 가지고 있다.\n",
        "- 유용한 단어이며, 문장을 관통하는 단어가 된다.\n",
        "- 문법적인 요소로, 판별에 필요한 정보는 없다.\n",
        "\n",
        "자주 등장하는 단어들의 가중치를 조절하는 방법으로 TF-IDF가 존재한다.\n",
        "\n",
        "단어 $t$, 문서 $d$에 대하여\n",
        "$$tf-idf(t,d) = tf(t,d) \\times idf(t, d)$$\n",
        "로 정의하게 될 것이다. 여기서\n",
        "\n",
        "- 문서 $d$에 등장한 단어 $t$의 횟수를 $tf(t, d)$\n",
        "- $$idf(t, d) = log \\frac{N}{1 + df(d, t)}$$\n",
        "where \n",
        "  - N : 전체 문서의 개수\n",
        "  - df(d,t) = 단어 t가 포함된 문서 d의 개수\n",
        "\n",
        "로 주어진다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvbGCk2vUs7T"
      },
      "source": [
        "### scikit-learn에서의 TF-IDF\n",
        "\n",
        "먼저 코드 예시를 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhlXPJKnTz0E",
        "outputId": "6f1efac5-b95a-4e8c-b065-446ee61d50f3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer(use_idf = True,\n",
        "                        norm = 'l2',  # L2 Norm을 의미한다. 12 아님.\n",
        "                        smooth_idf = True)\n",
        "np.set_printoptions(precision = 2)\n",
        "print(tfidf.fit_transform(count.fit_transform(corpus)).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.   0.39 0.56 0.46 0.   0.   0.   0.33 0.   0.46]\n",
            " [0.   0.24 0.   0.29 0.   0.   0.86 0.2  0.   0.29]\n",
            " [0.56 0.   0.   0.   0.   0.56 0.   0.27 0.56 0.  ]\n",
            " [0.   0.39 0.56 0.46 0.   0.   0.   0.33 0.   0.46]\n",
            " [0.   0.45 0.   0.   0.8  0.   0.   0.38 0.   0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4ZQOlR8V2n1"
      },
      "source": [
        "먼저 과정에서 norm이 들어가는데, 이는 정규화를 의미한다.\n",
        "보통 tf-idf를 계산하기 전, tf를 정규호 하는데  여기서는 tf-idf 값을 직접 정규화한다. 기본적으로 L2 정규화를 사용하는데,\n",
        "$$ v_{norm} = \\frac{v}{||v||_{2}}$$ \n",
        "로 계산하면 된다.  (즉 그냥 normalize)\n",
        "\n",
        "또, idf와 tf-idf 계산 방식도 약간 다르다.\n",
        "\n",
        "- $$idf(t, d) = log \\frac{1 + N}{1 + df(d, t)}$$\n",
        "- $$tf-idf(t,d) = tf(t,d) \\times (idf(t, d) + 1)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEUhgrdzYC_c"
      },
      "source": [
        "## 토큰화와 형태소 분석\n",
        "텍스트 문서를 각각의 낱개로 쪼개는 작업을 생각해야 할 것이다. 가장 쉬운 방법은 split일 것이다.\n",
        "\n",
        "그런데, 영어의 경우 변화형이 각각 들어간다면, 그걸 따로 세기보다는 \"기본형\"으로 통합해서 하나로 세고 싶다. 이를 위한 방법이 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3InyV-4Y_jb"
      },
      "source": [
        "파이썬의 NLTK 패키지를 통해 이를 처리할 수 있다.\n",
        "여러 패키지가 있는데\n",
        "\n",
        "- 어간 추출 알고리즘 PorterStemmer\n",
        "- 원형 추출 알고리즘 WordNetLemmatizer\n",
        "\n",
        "단순 토큰화, 어간 추출, 원형 추출의 차이를 알아보자자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUHDACZNMhaa",
        "outputId": "f4cfbb1f-0d42-4761-eb3d-5732ade5ebe6"
      },
      "source": [
        "# 단순 토큰화\n",
        "def tokenizer(text):\n",
        "  return text.split()\n",
        "\n",
        "tokenizer('flies could fly and thus they like flying')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['flies', 'could', 'fly', 'and', 'thus', 'they', 'like', 'flying']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Oub1-sKNGv6",
        "outputId": "b3f7ea69-000f-4c84-ab2b-25434f1dba00"
      },
      "source": [
        "# PoterStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "def tokenizer_porter(text):\n",
        "  return [porter.stem(word) for word in text.split()]\n",
        "tokenizer_porter('flies could fly and thus they like flying')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fli', 'could', 'fli', 'and', 'thu', 'they', 'like', 'fli']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz5Gmpk6OSRZ"
      },
      "source": [
        "PorterStemmer의 경우, 실제로 사용되지 않은 단어들이 나왔다...(thu라던가)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJPWgptDOd6f",
        "outputId": "44e52317-2fa2-42df-cb92-014c71420d39"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lm = WordNetLemmatizer()\n",
        "def tokenizer_lm(text):\n",
        "  return [lm.lemmatize(word, pos=\"v\") for word in text.split()]\n",
        "tokenizer_lm('flies could fly and thus they like flying')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fly', 'could', 'fly', 'and', 'thus', 'they', 'like', 'fly']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpRrSRSaPEMb"
      },
      "source": [
        "WordNetLemmatizer를 사용할 경우 원형을 얻을 수 있다."
      ]
    }
  ]
}