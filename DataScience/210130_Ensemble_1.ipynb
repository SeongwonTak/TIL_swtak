{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210130 Ensemble 1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNSSnrk1sbL0BEnsQ19lGl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongwonTak/TIL_swtak/blob/master/DataScience/210130_Ensemble_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX77NlVP7wj5"
      },
      "source": [
        "# 210130 Ensemble 1 (예습!)\r\n",
        "\r\n",
        "앙상블 기법에 대해 알아보고자 한다.\r\n",
        "앙상블 기법은, **다양한 모델을 결합**하여 개별 분류기보다 더 좋은 성능을 달성하는 것이 목표이다.\r\n",
        "\r\n",
        "1개의 분류기 예측보다, 더 많은 분류기에서 얻은 예측을 종합하는 것이 더 좋은 결과를 얻을 수 있을 것이라고 직관적으로 이해할 수 있다.\r\n",
        "\r\n",
        "앙상블을 쓰면 유리한 이유와 함께, 간단한 개요 및 random_forest에 대해서 알아보고자 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPFzOgad9ZnH"
      },
      "source": [
        "## 1. 앙상블 방법의 개요\r\n",
        "\r\n",
        "앙상블 방법의 경우는 $m$개의 분류기 $(C_{1}, C_{2}, ..., C_{m})$를 훈련시켜, 각 분류기 별로 예측을 실시한 다음, **다수결 투표**를 통해 최종 예측을 결정한다.\r\n",
        "\r\n",
        "$$ \\hat{y} = mode\\{C_{1}(x), C_{2}(x),...,C_{m}(x)\\}$$\r\n",
        "\r\n",
        "앙상블 방법이 더 좋은 이유는 개별 분류기법 보다 error가 날 가능성이 더 적기 때문이다.\r\n",
        "\r\n",
        "모든 분류기 $n$개들이 독립이라고 가정하면, \r\n",
        "$$ P(y>=k)=\\sum_{k}^{n}{}_{n}\\mathrm{C}_{k}\\epsilon^{k}(1-\\epsilon)^{n-k}$$\r\n",
        "\r\n",
        "(이항정리를 생각하면 당연하다)\r\n",
        "\r\n",
        "여기서 $\\epsilon$은 하나의 분류기에서 예측이 틀릴 확률이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5RV_PRaaIoY"
      },
      "source": [
        "## 2. 배깅과 부스팅의 개요\r\n",
        "\r\n",
        "앙상블 방법에서 기본적으로 다수결 투표를 기반으로 한다.\r\n",
        "조금 더 구체적으로 앙상블 방법에서 쓰이는 \r\n",
        "**'배깅'**과 **'부스팅'**에 대해 알아보자.\r\n",
        "\r\n",
        "### 배깅\r\n",
        "배깅이란, 원본 훈련 세트에서 bootstrap sample을 뽑아 각 훈련기에 훈련 데이터로 넣고, 다수결을 시키는 방식이다.\r\n",
        "bootstrap이란, 중복 데이터를 허용하는 방식이다.\r\n",
        "\r\n",
        "### 부스팅\r\n",
        "부스팅이란, 약한 학습기에 순차적으로 훈련 세트의 데이터를 집어넣는 방식이다.\r\n",
        "\r\n",
        "가장 유명한 방식은 **AdaBoost(에이다 부스트)**이다.\r\n",
        "에이다부스트의 방법을 보면 다음과 같다.\r\n",
        "\r\n",
        "- 훈련세트에서 중복을 허용하지 않고 랜덤한 부분집합을 뽑아 최초의 약한 학습기 $C_{1}$을 훈련시킨다.\r\n",
        "- 앞에서 뽑은 데이터를 제외하고 두번째 랜덤한 부분집합을 뽑은 후 앞에서 잘못 예측한 샘플의 절반을 더해서 $C_{2}$를 학습시킨다.\r\n",
        "- 두번 다 잘못 분류한 샘플로 학습을 시켜 $C_{3}$을 학습시킨다.\r\n",
        "- 세 학습기로 다수결 투표를 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgn58haLfEDm"
      },
      "source": [
        "## 3. 앙상블 학습의 예시 _ random forest\r\n",
        "\r\n",
        "배깅과 부스팅에 대해서 알아보기 전에 random_forest에 대해 먼저 알아보려고 한다.\r\n",
        "\r\n",
        "1. $n$개의 랜덤한 부트스트랩 샘플을 뽑는다.\r\n",
        "2. 부트스트랩 샘플에서 결정 트리를 학습한다.\r\n",
        "  - 중복을 허용하지 않고 랜덤하게 d개의 feature을 선택한다.\r\n",
        "  - 최선의 분할을 주는 특성을 사용하여 노드를 분할한다.\r\n",
        "3. 이 단계들을 $k$번 반복한다.\r\n",
        "4. 다수결 투표!\r\n",
        "\r\n",
        "scikit-learn에는 이미 패키지가 준비되어있다.\r\n",
        "따라서 직접 개별 분류기를 고생해서 만들 이유는 없다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BdmqU_LmUbb",
        "outputId": "e88d696e-2ef1-4a44-d91f-82d3bd141df9"
      },
      "source": [
        "# 코드의 예시\r\n",
        "import seaborn as sns\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "iris  = sns.load_dataset('iris')\r\n",
        "X = iris.drop('species', axis=1)\r\n",
        "y = iris['species']\r\n",
        "\r\n",
        "# 랜덤포레스트 부분\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "forest = RandomForestClassifier(criterion='gini',\r\n",
        "                                n_estimators=25,\r\n",
        "                                random_state=1,\r\n",
        "                                n_jobs=2)\r\n",
        "result = forest.fit(X, y)\r\n",
        "forest.predict(X)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
              "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
              "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
              "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
              "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
              "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
              "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
              "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
              "       'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
              "       'virginica', 'virginica', 'virginica'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SovBRB_jpKtD"
      },
      "source": [
        "실제 결과에 대한 분석 및 앙상블에 대한 자세한 내용은 내일 데이터다이빙 수업시간에 배운다.\r\n",
        "미리 좀 접해보았고, 수업에서 자세히 배운 후 추가 내용과 참고도서의 내용을 추가하여 Ensemble 2편을 만들려고 한다."
      ]
    }
  ]
}