{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac81d7e",
   "metadata": {},
   "source": [
    "# LDA - Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a720174",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation.에 대해서 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b5841",
   "metadata": {},
   "source": [
    "## sklearn을 활용한 LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e8e40",
   "metadata": {},
   "source": [
    "sklearn을 활용한 방법이다.\n",
    "- sklearn.decomposition에 LatentDirichletAllocation이 포함되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa81fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "715de12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_select = ['sci.space', 'rec.sport.baseball', 'comp.graphics', 'soc.religion.christian']\n",
    "news_df = fetch_20newsgroups(remove = ('headers', 'footers', 'quotes'),\n",
    "                             subset = 'all',\n",
    "                             categories = cats_select,\n",
    "                             random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fcc07d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\tThis happens when your X server has run out of memory. You need\\nmore memory or you need to quit any un-neccessary running clients.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5f38e",
   "metadata": {},
   "source": [
    "이런 식으로, news_df.data를 통해 각 기사에 접근할 수 있다. 이 기사들의 주제를 분류하기 위해, \n",
    "- countvectorizer을 통해, 단어 빈도수 기반 벡터화 진행\n",
    "- 이후, LDA 피처 벡터화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d29668b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_df = 0.95, max_features = 1000,\n",
    "                       min_df = 2, stop_words = 'english')\n",
    "news_vected = vect.fit_transform(news_df.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587fb49",
   "metadata": {},
   "source": [
    "이제, 여기서 LDA를 적용하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b433dd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1000)\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 4, random_state = 42)\n",
    "lda.fit(news_vected)\n",
    "\n",
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc687c9",
   "metadata": {},
   "source": [
    "fit은 토픽별 단어 분포를 계산하는 과정이고,\n",
    "transform에서는 문서별 토픽들의 분포까지 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d61f6620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.72236418e-01, 2.30927737e+01, 2.54495169e-01, ...,\n",
       "        3.23602182e+01, 6.14361900e+01, 2.68063104e-01],\n",
       "       [2.78103337e+02, 1.95116709e+02, 1.63189865e+02, ...,\n",
       "        8.60379744e+01, 1.46090198e+01, 2.52322788e-01],\n",
       "       [2.56436984e-01, 2.63107838e-01, 2.50040966e-01, ...,\n",
       "        2.97855510e+00, 3.87606706e+01, 2.50019674e-01],\n",
       "       [4.82679898e+01, 6.52740995e+00, 1.30559843e+00, ...,\n",
       "        1.26232523e+01, 2.19411954e+00, 1.26229594e+02]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc1e7bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03314266 0.03161752 0.03168517 0.90355465]\n",
      " [0.03278311 0.03256219 0.03202135 0.90263335]\n",
      " [0.2485936  0.46555658 0.00563905 0.28021078]\n",
      " ...\n",
      " [0.80912438 0.06381262 0.06390989 0.06315311]\n",
      " [0.00946124 0.00989854 0.00929883 0.97134139]\n",
      " [0.18965096 0.0017655  0.80681486 0.00176867]]\n"
     ]
    }
   ],
   "source": [
    "document_topics = lda.transform(news_vected)\n",
    "print(document_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cab26a",
   "metadata": {},
   "source": [
    "이렇게, 각 문서별로 어떤 주제가 나올 가능성이 가장 높은지 return해주게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01fcab",
   "metadata": {},
   "source": [
    "## gensim을 활용한 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
