{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLScratch_ch5_prepare.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdosEjac/4Ta47Ok9CVQ89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongwonTak/TIL_swtak/blob/master/DLScratch1Study/DLScratch_ch5_prepare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U7k4iYWy4XW"
      },
      "source": [
        "# 밑바닥부터 시작하는 딥러닝_5장 스터디 준비\r\n",
        "어느 덧, 스터디 4주차.  5장 스터디를 준비한 내용을 정리한다.\r\n",
        "(내용이 상당히 어렵더라..)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSXkrInbzAIG"
      },
      "source": [
        "## 5.1 계산 그래프\r\n",
        "### 계산그래프란?\r\n",
        "- 계산과정을 그래프로 표현\r\n",
        "- 여기서 말하는 그래프는 node와 edge로 표현하는 그림.\r\n",
        "- 계산과정을 왼쪽에서 오른쪽으로 진행하면 (forward propagation)\r\n",
        "- 계산과정을 오른쪽에서 왼쪽으로 역방향 진행하면 (backword propagation)\r\n",
        "  - 역전파는 미분을 계산할때 중요한 역할을 한다.(역전파가 미분을 전달하게된다)\r\n",
        "\r\n",
        "**질문사항, 대체 왜 역함수가 아닌 미분을 역전파로 전달하는거를 생각하는걸까?**\r\n",
        "\r\n",
        "### 계산그래프의 예시\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCv-Ufhgz0AV"
      },
      "source": [
        "## 5.2 chain rule\r\n",
        "- 계산 그래프의 역전파\r\n",
        "순전파때의 계산의 미분을 구한다.\r\n",
        "\r\n",
        "\r\n",
        "### chain rule(미분 표현)\r\n",
        "$z=f(t)$\r\n",
        "$t=f(x)$\r\n",
        "일 경우,\r\n",
        "$$\\frac{\\partial z}{\\partial x}=\\frac{\\partial z}{\\partial t}\\frac{\\partial t}{\\partial x}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBX5Wy3k1xrg"
      },
      "source": [
        "## 5.3~5.4 덧셈노드와 곱셈노드의 역전파 \r\n",
        "\r\n",
        "### 덧셈노드\r\n",
        "만일 $z = x+y$라면,\r\n",
        "$$ \\frac{\\partial z}{\\partial x}=1$$\r\n",
        "$$ \\frac{\\partial z}{\\partial y}=1$$\r\n",
        "이므로, 덧셈 노드에 대해서 역전파를 보낼 경우에는 같은 값을 그대로 다음 노드에 전달하게 된다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVrajUTW4OZZ"
      },
      "source": [
        "class AddLayer:\r\n",
        "  def __init__(self):\r\n",
        "    pass\r\n",
        "  \r\n",
        "  def forward(self, x, y):\r\n",
        "    out = x + y\r\n",
        "    return out\r\n",
        "\r\n",
        "  def backward(self, dout):\r\n",
        "    dx = dout * 1\r\n",
        "    dy = dout * 1\r\n",
        "    return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7NBQKJ7AI_G"
      },
      "source": [
        "## 곱셈노드\r\n",
        "만일 $z = xy$라면,\r\n",
        "$$ \\frac{\\partial z}{\\partial x}=y$$\r\n",
        "$$ \\frac{\\partial z}{\\partial y}=x$$\r\n",
        "이다.즉, 곱셈 노드 역전파는 순전파 때의 입력 신호들을 **서로 바꾼 값을 곱해서** 하류로 보낸다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lFdKKb4d1Cc"
      },
      "source": [
        "class MulLayer:\r\n",
        "  def __init__(self):\r\n",
        "    self.x = None\r\n",
        "    self.y = None\r\n",
        "  \r\n",
        "  def forward(self, x, y):\r\n",
        "    self.x = x\r\n",
        "    self.y = y\r\n",
        "    out = x * y\r\n",
        "  \r\n",
        "  def backward(self, dout):\r\n",
        "    dx = dout * self.y\r\n",
        "    dy = dout * self.x\r\n",
        "\r\n",
        "    return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8lA9SBJezXP"
      },
      "source": [
        "### 예시"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNS6p8vte3Jr"
      },
      "source": [
        "## 5.5 활성화 함수 계층 구현\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w96NvfSgmz_P"
      },
      "source": [
        "\r\n",
        "### ReLU\r\n",
        "ReLU 함수를 미분하면, 0보다 클때는 1, 아닐때는 0으로 나올것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L7NhZ55i2EL"
      },
      "source": [
        "class Relu:\r\n",
        "  def __init__(self):\r\n",
        "    self.mask = None\r\n",
        "  \r\n",
        "  def forward(self, x):\r\n",
        "    self.mask = (x <= 0)\r\n",
        "    out = x.copy()\r\n",
        "    out[self.mask] = 0\r\n",
        "\r\n",
        "    return out\r\n",
        "\r\n",
        "  def backward(self, dout):\r\n",
        "    dout[self.mask] = 0\r\n",
        "    dx = dout\r\n",
        "  \r\n",
        "    return dx\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw5w5yEDlaQV"
      },
      "source": [
        "### sigmoid\r\n",
        "sigmoid의 경우는 4단계로 나누어서 함수 계산을 한다.\r\n",
        "\r\n",
        "x -> -x -> exp(-x) -> 1+exp(-x) -> sigmoid\r\n",
        "\r\n",
        " 즉 * -1,  exp,  +1,  역수의 과정으로 연산을 한 것이며 각 함수를 미분하여 역으로 돌려주는 과정을 역전파로 구한다.\r\n",
        "\r\n",
        " - 역전파 1단계 : $y=\\frac{1}{x}$의 역전파 구하기\r\n",
        " - 역전파 2단계 : 덧셈의 역전파\r\n",
        " - 역전파 3단계 : $y = exp(x)$의 역전파 구하기\r\n",
        " - 역전파 4단계 : 곱셈의 역전파\r\n",
        "\r\n",
        " 위와 동일한 식으로, 미분을 구하여 곱하는 형태로 역전파를 구한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCDIrcsfmtLH"
      },
      "source": [
        "class sigmoid:\r\n",
        "  def __init__(self):\r\n",
        "    self.out = None\r\n",
        "  \r\n",
        "  def forward(self, x):\r\n",
        "    out = 1 / (1 + np.exp(-x))\r\n",
        "    self.out = out\r\n",
        "\r\n",
        "    return out\r\n",
        "  \r\n",
        "  def backward(self, dout):\r\n",
        "    dx = dout* (1.0 - self.out) * self.out\r\n",
        "    \r\n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23jAgZDjibV"
      },
      "source": [
        "## 5.6 Affine 계층과 Softmax 계층\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyRp_-mEnNqU"
      },
      "source": [
        "\r\n",
        "### Affine 계층\r\n",
        "앞에서 배운 신경망의 순전파는 행렬의 곱이다.\r\n",
        "이에 대한 역전파를 고려하자.\r\n",
        "\r\n",
        "$X \\cdot W$의 미분을 고려하는 것으로 충분하다.\r\n",
        "(상수항의 덧셈부분, 덧셈의 역전파는 그대로 전달한다는 사실을 잊지 말자)\r\n",
        "\r\n",
        "$$\\frac{\\partial L}{\\partial X}=\\frac{\\partial L}{\\partial Y} \\cdot W^{T} $$\r\n",
        "\r\n",
        "$$\\frac{\\partial L}{\\partial W}=X^{T}\\cdot \\frac{\\partial L}{\\partial Y}  $$\r\n",
        "\r\n",
        "역전파 또한 배열로 주어진다.\r\n",
        "\r\n",
        "Batch Model을 고려해도 결과에는 변화가 없다.\r\n",
        "\r\n",
        "**Remark. 행렬의 미분에 대한 내용 확인 필요**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHbIdy-UmVvL"
      },
      "source": [
        "class Affine:\r\n",
        "  def __init__(self, W, b):\r\n",
        "    self.W = W\r\n",
        "    self.b = b\r\n",
        "    self.x = None\r\n",
        "    self.dW = None\r\n",
        "    self.db = None\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    self.x = x\r\n",
        "    out = np.dot(x, self.W) + self.b\r\n",
        "\r\n",
        "    return uot\r\n",
        "\r\n",
        "  def backward(self, dout):\r\n",
        "    dx = np.dot(dout, self.W.T)\r\n",
        "    self.dw = np.dot(self.x.T, dout)\r\n",
        "    self.db = np.sum(dout, axis=0)\r\n",
        "\r\n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubfTcLA2nOm0"
      },
      "source": [
        "### Softmax 역전파\r\n",
        "(증명은 추후 기술)\r\n",
        "출력이 $(y_{1},y_{2},y_{3})$이고 정답이 $(t_{1},t_{2},t_{3})$일때, 역전파의 결과는 두 벡터의 차이이다.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrOEuFY1oJwT"
      },
      "source": [
        "class SoftmaxWithLoss:\r\n",
        "  def __init__(self):\r\n",
        "    self.loss = None\r\n",
        "    self.y = None\r\n",
        "    self.t = None\r\n",
        "  \r\n",
        "  def forward(self, x, t):\r\n",
        "    self.t = t\r\n",
        "    self.y = softmax(x)\r\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\r\n",
        "    return self.loss\r\n",
        "  \r\n",
        "  def backward(self, dout = 1):\r\n",
        "    batch_size = self.t.shape[0]\r\n",
        "    dx = (self.y - self.t)/batch_size\r\n",
        "\r\n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "minbGVYPI0Y3"
      },
      "source": [
        "## 5.7 오차 역전파법의 구현\r\n",
        "\r\n",
        "- 신경망 학습의 전체 그림은 동일함\r\n",
        "미니배치 -> 기울기산출 -> 매개변수의 갱신을 반복."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcZyjbaBJDMc"
      },
      "source": [
        "# 오차역전파법이 적용된 신경망\r\n",
        "import sys, os\r\n",
        "sys.path.append(os.pardir)\r\n",
        "import numpy as np\r\n",
        "from common.layers import *\r\n",
        "from common.gradient import numerical_gradient\r\n",
        "from collections import OrderedDict\r\n",
        "\r\n",
        "class TwoLayerNet:\r\n",
        "  def __init__(self, input_size, hidden_size, output_size,\r\n",
        "               weight_init_std = 0.01):\r\n",
        "    self.params = {}\r\n",
        "    self.params['W1'] = weigit_init_std * np.random.randn(input_size, hidden_size)\r\n",
        "    self.params['b1'] = np.zeros(hidden_size)\r\n",
        "    self.params['W2'] = weigit_init_std * np.random.randn(input_size, hidden_size)\r\n",
        "    self.params['b2'] = np.zeros(output_size)\r\n",
        "\r\n",
        "    # 계층의 생성\r\n",
        "    self.layers = OrderedDict()\r\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\r\n",
        "    self.layers['Relu1'] = Relu()\r\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\r\n",
        "\r\n",
        "    self.lastLayer = SoftmaxWithLoss()\r\n",
        "\r\n",
        "  def predict(self, x):\r\n",
        "    for layer in self.layers.values():\r\n",
        "      x = layer.forward(x)\r\n",
        "\r\n",
        "    return x\r\n",
        "\r\n",
        "  def loss(self, x, t):\r\n",
        "    y = self.predict(x)\r\n",
        "    return self.lastLayer.forward(y, t)\r\n",
        "\r\n",
        "  def accuracy(self, x, t):\r\n",
        "    y = self.predict(x)\r\n",
        "    y = np.argmax(y, axis = 1)\r\n",
        "    if t.ndim != 1 : t = np.argmax(t, axis = 1)\r\n",
        "\r\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "  def numerical_gradient(self, x, t):\r\n",
        "    loss_W = lambda W : self.loss(x, t)\r\n",
        "\r\n",
        "    grads = {}\r\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\r\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\r\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\r\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\r\n",
        "    return grads\r\n",
        "\r\n",
        "  def gradient(self, x, t):\r\n",
        "    self.loss(x, t)  # 순전파\r\n",
        "\r\n",
        "    dout = 1\r\n",
        "    dout = self.lastLayer.backward(dout)\r\n",
        "\r\n",
        "    layers = list(self.layers.values())\r\n",
        "    layers.reverse()\r\n",
        "    for layer in layers:\r\n",
        "      dout = layer.backward(dout)\r\n",
        "\r\n",
        "    grads = {}\r\n",
        "    grads['W1'] = self.layers['Affine1'].dW\r\n",
        "    grads['b1'] = self.layers['Affine1'].db\r\n",
        "    grads['W2'] = self.layers['Affine2'].dW\r\n",
        "    grads['b2'] = self.layers['Affine2'].db\r\n",
        "\r\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}