{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLscratch_ch4_prepare.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKPJkHLEGPuE3CwdkqcmaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongwonTak/TIL_swtak/blob/master/DLScratch1Study/DLscratch_ch4_prepare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILB9gP1mxkAD"
      },
      "source": [
        "#210211 밑바닥부터 시작하는 딥러닝 4장 정리\r\n",
        "이번주에 있을 4장 스터디 내용을 정리한다.\r\n",
        "\r\n",
        "참고도서 : 밑바닥부터 시작하는 딥러닝 4장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QonTLFN9xr1F"
      },
      "source": [
        "## 4.2 손실함수(loss function)\r\n",
        "\r\n",
        "- 신경망의 현 상태를 나타내는 지표를 loss functuon으로 표현하게 될 것이다.\r\n",
        "  - '**손실**'함수라는 이름에서 보듯 신경망이 얼마나 나쁜지에 대해 논의하는 지표이다.\r\n",
        "\r\n",
        "- 대표적인 손실 함수는 오차제곱합과 엔트로피.\r\n",
        "(물론 새로 정의하는 경우도 많을 것이지만)\r\n",
        "\r\n",
        "### 오차제곱합 (Sum of squares for error, SSE)\r\n",
        "(어째선지는 모르겠지만, 책에서는 굳이 0.5를 곱한다)\r\n",
        "\r\n",
        "$$ E = \\frac{1}{2}\\sum_{k}(y_{k}-t_{k})^{2}$$\r\n",
        "\r\n",
        "\r\n",
        "실제 사용에서는 오차제곱합이 가장 적은 것을 정답으로 추정해볼 수 있을 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhIEYexBw06A"
      },
      "source": [
        "# SSE의 구현\r\n",
        "import numpy as np\r\n",
        "def sum_squares_error(y, t):\r\n",
        "  return 0.5 * np.sum((y-t)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb3z-YPjzJMC",
        "outputId": "c5669319-22df-4498-c743-1ff8d828f4d5"
      },
      "source": [
        "# 비교 예시\r\n",
        "t = [0, 0, 1, 0, 0]  # 참값. 정답은 2라고 생각\r\n",
        "y1 = [0.1, 0.1, 0.6, 0.15, 0.05]  # 2일 확률을 최대로 예상한 예시\r\n",
        "y2 = [0.6, 0.1, 0.1, 0.15, 0.05]  # 0일 확률을 최대로 예상한 예시\r\n",
        "\r\n",
        "print(sum_squares_error(np.array(y1), np.array(t)))\r\n",
        "print(sum_squares_error(np.array(y2), np.array(t)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10250000000000002\n",
            "0.6025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_9IenK60DfY"
      },
      "source": [
        "### 교차 엔트로피 오차 (Cross Entropy Error, CEE)\r\n",
        "$$ E = -\\sum_{k}t_{k}log \\;y_{k}$$\r\n",
        "여기서 실제 참값인 $t_{k}$는 One-hot Encoding이 적용된다.\r\n",
        "로그를 사용하기에 로그에 들어가야 하는 값은 양수이므로, 실제 구현에서는 매우 작은 값을 더하게 된다.\r\n",
        "\r\n",
        "확률을 기준으로 출력하기에 1보다 작은 값이고, 더 작은 값이 맞다고 예측할 경우 로그의 성질에 의해, 엔트로피 값이 커질 것이다.\r\n",
        "\r\n",
        "엔트로피(불순도)가 낮을수록 더 좋은 예측이다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0e-jX7B4Rvg"
      },
      "source": [
        "# CEE의 구현\r\n",
        "def cross_entropy_error(y, t):\r\n",
        "  delta = 1e-7\r\n",
        "  return -np.sum(t * np.log(y+delta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byQuXkh-5fe1",
        "outputId": "a6c73fc8-235f-4e98-e645-6ab3d0629ea0"
      },
      "source": [
        "# CEE의 예시, 위와 동일한 수치 이용\r\n",
        "print(cross_entropy_error(np.array(y1), np.array(t)))\r\n",
        "print(cross_entropy_error(np.array(y2), np.array(t)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.510825457099338\n",
            "2.302584092994546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7LF4leq6M3B"
      },
      "source": [
        "### 미니배치\r\n",
        "\r\n",
        "훈련 데이터가 1개라는 보장은 없다.. 훈련 데이터가 수십만, 수백만 개인 경우를 생각하면 전체를 대상으로 손실함수를 계산하는 것은 무리일 것이다.\r\n",
        "훈련 데이터 중에서 일부만을 골라 학습을 수행하는 것을** mini-batch** 라고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy4Sdg7vMbFd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "a839021f-f509-465b-b1af-7455f894716b"
      },
      "source": [
        "# Mini-Batch의 예시\r\n",
        "train_size = x_train.shape[0]\r\n",
        "bath_size = 10\r\n",
        "batch_mark = np.random.choice(train_size, batch_size)\r\n",
        "x_batch = x_train[batch_mark]\r\n",
        "t_batch = t_train[batch_mark]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-2f9a045a65a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mini-Batch의 예시\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbath_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_mark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEogoMbaMaKa"
      },
      "source": [
        "이제 배치된 데이터 셋의 교차 엔트로피 오차를 구할 수 있을 것이다.\r\n",
        "이 경우, 각 데이터 별로의 교차 엔트로피의 평균을 구하게 된다.\r\n",
        "$$ E = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\;log\\;y_{nk}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXyoWE-Z6OWn"
      },
      "source": [
        "## 4.3 수치 미분\r\n",
        "당연히, 일반적인 함수 미분은 유감스럽게도 구현이 곤란하다.\r\n",
        "실제 사용에서는 오차함수의 최적화를 위해서는 미분을 활용해야 한다.\r\n",
        "\r\n",
        "### 미분과 수치 미분의 정의\r\n",
        "일반적인 미분의 정의는 알다시피,\r\n",
        "$$ \\frac{df(x)}{dx} = \\lim_{h\\rightarrow0}\\frac{f(x+h)-f(x)}{h}$$\r\n",
        "\r\n",
        "하지만, 컴퓨터상에서는\r\n",
        "- 이 limit를 실제로 구현하는 것이 불가능!\r\n",
        "- 하다못해 정말 작은 h, 막 예를 들어 1e-50을 쓰고 싶어도.. 반올림문제로 오차를 만든다.  h=1e-04 정도로 사용한다.\r\n",
        "\r\n",
        "즉, 이 경우 미분이 아닌 실제로는 **차분** 개념을 사용하게 된다.\r\n",
        "\r\n",
        "그럼, 위의 식대로 사용하면 정녕 괜찮은건가?\r\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY6a_Zkn89fj"
      },
      "source": [
        "그림을 보면 알겠지만, 근사로 구한 접선의 경우는 $x$와 $x+h$ 좌표 사이의 기울기를 뜻하지만.. 접선과는 차이가 크다.\r\n",
        "\r\n",
        "애초에 수치미분에는 오차가 포함되지만, 이 오차를 줄이기 위해서는 우측 방향에서만 값을 줄이는 것이 아닌, 양측에서 h값을 줄이는 방식 또한 사용이 가능할 것이다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLjkBzto9Tgv"
      },
      "source": [
        "# 실제 수치 미분의 구현\r\n",
        "def numerical_diff(f, x):\r\n",
        "  h = 1e-04\r\n",
        "  return (f(x+h)-f(x-h))/(2*h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvryMWj9hbY",
        "outputId": "530b774f-21ce-4829-c2fd-7476d24a73e9"
      },
      "source": [
        "# 수치 미분의 예시\r\n",
        "def ftn (x):\r\n",
        "  return x ** 2 - x\r\n",
        "\r\n",
        "numerical_diff(ftn, 5)  # 실제 정답은 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.999999999979025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG0l1jQi95S4"
      },
      "source": [
        "### 편미분의 구현\r\n",
        "다변수 함수의 미분의 경우는 어떤 변수에 대해 미분할 것인가가 중요할 것이다.\r\n",
        "특정 지점에서의 편미분 값을 구하기 위해서는 미분하려는 하나의 변수를 제외하고 나머지를 고정시키면 쉽게 해결된다.\r\n",
        "\r\n",
        "문제) $f(x, y) = x^{2}+y^{2}$ 에 대해, $ x = 3, y = 4 $ 지점에서 각 변수의 수치 편미분은?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVDeKr8O_LQo",
        "outputId": "ed92e8f3-4ed6-41ef-bc46-90a5bd800369"
      },
      "source": [
        "def ftn_1 (x0):\r\n",
        "  return x0 ** 2 + 4 ** 2\r\n",
        "\r\n",
        "numerical_diff(ftn_1, 3)  # 실제 정답은 6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.00000000000378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQmgYx1n_Zrj",
        "outputId": "c88a0cdd-ee71-4593-d029-18e2661968c5"
      },
      "source": [
        "def ftn_2 (y0):\r\n",
        "  return 3 ** 2 + y0 ** 2\r\n",
        "\r\n",
        "numerical_diff(ftn_2, 4)  # 실제 정답은 8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.999999999999119"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYP8CQ4b_m6P"
      },
      "source": [
        "## 4.4 기울기와 경사하강법\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q29DQkjvQCy0"
      },
      "source": [
        "### Gradient\r\n",
        "모든 변수의 편미분의 벡터 표현을 Gradient라고 하며, 이는 다변수 함수에서 기울기 역할을 하게 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL9AlPf8QPxw"
      },
      "source": [
        "def numerical_gradient(f, x):\r\n",
        "  h = 1e-04\r\n",
        "  grad = np.zeros_like(x)\r\n",
        "\r\n",
        "  for idx in range(x.size):\r\n",
        "    tmp_val = x[idx]\r\n",
        "    x[idx] = tmp_val + h\r\n",
        "    fxh1 = f(x)\r\n",
        "\r\n",
        "    x[idx] = tmp_val - h\r\n",
        "    fxh2 = f(x)\r\n",
        "\r\n",
        "    grad[idx] = (fxh1-fxh2) / (2*h)\r\n",
        "    x[idx] = tmp_val\r\n",
        "  \r\n",
        "  return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ1v2QckNLEy"
      },
      "source": [
        "### 경사하강법\r\n",
        "경사하강법은 rough하게는 기울기를 사용하여 함수의 최솟값을 찾는 방법이다.\r\n",
        "( 기울기가 가리키는 곳에 함수의 최솟값이라는 보장은 없다. 정확히는 극값이거나 안장점일 것이다)\r\n",
        "\r\n",
        "기울어진 방향은 함수의 값을 줄일 수 있는 방향을 준다.\r\n",
        "최솟값을 찾는 경우는 Gradient Descent Method,\r\n",
        "최댓값을 찾는 경우는 Gradient Ascent Method.\r\n",
        "라고 부른다.\r\n",
        "\r\n",
        "경사하강법은, 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다.\r\n",
        "기울어진 방향으로 조금씩 나아가며 함수의 값을 점차 줄이거나 늘린다.\r\n",
        "\r\n",
        "수식으로 표현할 경우 다음가 같다.\r\n",
        "\r\n",
        "$$ x_{0} = x_{0} - \\eta\\frac{\\partial f}{\\partial x_{0}}$$\r\n",
        "$$ x_{1} = x_{1} - \\eta\\frac{\\partial f}{\\partial x_{1}}$$\r\n",
        "\r\n",
        "위를 여러 번 반복하게 된다. 여기서 $\\eta$는 학습률이 된다.\r\n",
        "(이 학습률이 너무 크면 발산하기 쉽고, 너무 작으면 함수값이 제대로 갱신될 수 없다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ1bQlbJh-0x"
      },
      "source": [
        "def gradient_descent(f, init_x, lr, step_num):\r\n",
        "  x = init_x\r\n",
        "\r\n",
        "  for i in range(step_num):\r\n",
        "    grad = numerical_gradient(f, x)\r\n",
        "    x -= lr*grad\r\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "911NsUn6iR6N",
        "outputId": "41708f41-a734-4cfb-a599-c092d8639a2c"
      },
      "source": [
        "# example\r\n",
        "def function_2(x):\r\n",
        "  return x[0]**2 + x[1]**2\r\n",
        "\r\n",
        "init_x = np.array([-3.0, -4.0]) # 시작점\r\n",
        "print(gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num=5))\r\n",
        "print(gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num=100 ))\r\n",
        "print(gradient_descent(function_2, init_x = init_x, lr = 300, step_num=100 ))\r\n",
        "# 정답 : (0, 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.98304 -1.31072]\n",
            "[-2.00248785e-10 -2.66998379e-10]\n",
            "[-3.31221939e+12 -4.44086855e+12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqGzpA-jiRG_"
      },
      "source": [
        "### 신경망에서의 기울기\r\n",
        "\r\n",
        "신경망에서의 기울기는 가중치 매개변수에 대한 손실 함수의 기울기를 의미한다.\r\n",
        "손실함수를 각 가중치에 대해 미분하고 이를 행렬로 표현하게 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS4Jv9pHmFM8"
      },
      "source": [
        "# 소프트맥스의 구현(from ch3)\r\n",
        "\r\n",
        "def softmax(a):\r\n",
        "  c = np.max(a)\r\n",
        "  exp_a = np.exp(a-c) # 오버플로 막기\r\n",
        "  sum_exp_a = np.sum(exp_a)\r\n",
        "  y = exp_a / sum_exp_a\r\n",
        "\r\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TustjIJMmILc"
      },
      "source": [
        "# class 구현\r\n",
        "class simpleNet:\r\n",
        "  def __init__(self):\r\n",
        "    self.W = np.random.randn(2, 3)\r\n",
        "  \r\n",
        "  def predict(self, x):\r\n",
        "    return np.dot(x, self.W)\r\n",
        "\r\n",
        "  def loss(self, x, t):\r\n",
        "    z = self.predict(x)\r\n",
        "    y = softmax(z)\r\n",
        "    loss = cross_entropy_error(y, t)\r\n",
        "\r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbP2HCr0mfkl",
        "outputId": "95a38071-f88d-49ce-f7b6-8e2f541db7de"
      },
      "source": [
        "# example_samplenet\r\n",
        "net = simpleNet()\r\n",
        "print(net.W)\r\n",
        "\r\n",
        "x = np.array([0.6, 0.9])\r\n",
        "p = net.predict(x)\r\n",
        "print(p)\r\n",
        "t = np.array([0, 0, 1])\r\n",
        "net.loss(x, t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.93343646 -0.38807755 -0.91693233]\n",
            " [ 0.64469299  0.06308762 -0.3179356 ]]\n",
            "[ 0.02016181 -0.17606768 -0.83630144]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6658294774158389"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFyp6pxHnC2n"
      },
      "source": [
        "# example_gradient\r\n",
        "\r\n",
        "def f(w):\r\n",
        "  return net.loss(x, t)\r\n",
        "\r\n",
        "dW = numerical_gradient(f, net.W)  # 왜 오류가 발생?  # f는 뭐고.\r\n",
        "print(dW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j_fCaB1nfNP"
      },
      "source": [
        "## 4.5 학습 알고리즘의 구현\r\n",
        "\r\n",
        "### 신경망 학습의 절차\r\n",
        "신경망에 적응 가능한 가중치와 편향이 있고, 이를 훈련 데이터에 적응하도록 조정하는 과정인 '학습'을 할 수 있어야 한다.\r\n",
        "\r\n",
        "- 1단계 : Mini Batch\r\n",
        "  - 훈련데이터 중 일부를 무작위로 가져옴. 목표는 Mini Batch의 손실함수값 줄이기\r\n",
        "- 2단계 : gradient 산출\r\n",
        "  - 가중치 매개변수의 기울기를 구한다.\r\n",
        "  - 기울기는 손실함수 값을 가장 작게 하는 방향을 제시한다.\r\n",
        "- 3단계 : 매개변수의 갱신\r\n",
        "  - Gradient descent를 활용하여 가중치 매개변수를 갱신\r\n",
        "- 4단계 : 위의 과정을 반복한다.\r\n",
        "\r\n",
        "Mini Batch 과정에서 데이터를 무작위로 선정하기에, 일반적인 경사 하강법이 아닌, **stochastic gradient descent (확률적 경사 하강법)** 이라고 한다.\r\n",
        "\r\n",
        "실제 코드를 통해 자세한 구현을 알아보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI73VH6xqy5o"
      },
      "source": [
        "# 2층 신경망 클래스의 구현\r\n",
        "\r\n",
        "class TwoLayerNet:\r\n",
        "  def __init__ (self, input_size, hidden_size, output_size, \r\n",
        "                weight_init_std=0.01):\r\n",
        "    # 가중치의 초기화\r\n",
        "    # params : 신경망의 매개변수를 보관함 (가중치, 편향)\r\n",
        "    self.params = {}\r\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\r\n",
        "    self.params['b1'] = np.zeros(hidden_size)\r\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\r\n",
        "    self.params['b2'] = np.zeros(output_size)\r\n",
        "\r\n",
        "  def predict(self, x):  \r\n",
        "    # 기본적인 이층 신경망대로.\r\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\r\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\r\n",
        "\r\n",
        "    a1 = np.dot(x, W1) + b1\r\n",
        "    z1 = sigmoid(a1)\r\n",
        "    a2 = np.dot(z1, W2) + b2\r\n",
        "    y = softmax(a2)\r\n",
        "\r\n",
        "    return y\r\n",
        "\r\n",
        "  def loss(self, x, t):\r\n",
        "    y = self.predict(x)\r\n",
        "\r\n",
        "    return cross_entropy_error(y, t)\r\n",
        "  \r\n",
        "  def accuracy(self, x, t):\r\n",
        "    y = self.predict(x)\r\n",
        "    y = np.argmax(y, axis = 1)\r\n",
        "    t = np.argmax(t, axis = 1)\r\n",
        "\r\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "  def numerical_gradient(self, x, t):\r\n",
        "    loss_W = lambda W: self.loss(x, t)\r\n",
        "\r\n",
        "    grads = {}  # 기울기를 보관함\r\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\r\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\r\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\r\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\r\n",
        "\r\n",
        "    return grads\r\n",
        "\r\n",
        "  def gradient(self, x, t):\r\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\r\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\r\n",
        "        grads = {}\r\n",
        "        \r\n",
        "        batch_num = x.shape[0]\r\n",
        "        \r\n",
        "        # forward\r\n",
        "        a1 = np.dot(x, W1) + b1\r\n",
        "        z1 = sigmoid(a1)\r\n",
        "        a2 = np.dot(z1, W2) + b2\r\n",
        "        y = softmax(a2)\r\n",
        "        \r\n",
        "        # backward\r\n",
        "        dy = (y - t) / batch_num\r\n",
        "        grads['W2'] = np.dot(z1.T, dy)\r\n",
        "        grads['b2'] = np.sum(dy, axis=0)\r\n",
        "        \r\n",
        "        da1 = np.dot(dy, W2.T)\r\n",
        "        dz1 = sigmoid_grad(a1) * da1\r\n",
        "        grads['W1'] = np.dot(x.T, dz1)\r\n",
        "        grads['b1'] = np.sum(dz1, axis=0)\r\n",
        "\r\n",
        "        return grads"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSX1p6wLtHD_"
      },
      "source": [
        "### 시험 데이터를 통한 평가\r\n",
        "\r\n",
        "특정 시점부터 손실함수의 값이 서서히 내려감을 알 수 있다.\r\n",
        "신경망의 학습에서는 훈련데이터 외의 데이터를 올바르게 인식하는지를 알아야 한다.\r\n",
        "다른말로 'Overfitting'이 일어나지 않는지 확인해야 한다.\r\n",
        "\r\n",
        "Remark. 에폭(epoch) \r\n",
        "에폭은 하나의 단위로 1에폭이란 훈련데이터를 모두 소진했을때의 횟수에 해당."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWG-WiZY_xLr"
      },
      "source": [
        "# 이놈의 import문제는 대체 어떻게 해결해야 하는건가요?\r\n",
        "# 구글에서 찾아본 그 어떤 방법도 제 컴퓨터에서는 안먹히네요.\r\n",
        "# 짜증나서 그냥 mnist를 박아버린다.\r\n",
        "\r\n",
        "import sys, os\r\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\r\n",
        "\r\n",
        "# from tensorflow.keras.datasets.mnist import load_data"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUuYJ6KeXfe-",
        "outputId": "4e071dfe-4a15-49d8-d096-1056768b9ca9"
      },
      "source": [
        "# coding: utf-8\r\n",
        "try:\r\n",
        "    import urllib.request\r\n",
        "except ImportError:\r\n",
        "    raise ImportError('You should use Python 3.x')\r\n",
        "import os.path\r\n",
        "import gzip\r\n",
        "import pickle\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "url_base = 'http://yann.lecun.com/exdb/mnist/'\r\n",
        "key_file = {\r\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\r\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\r\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\r\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\r\n",
        "}\r\n",
        "\r\n",
        "dataset_dir = os.path.dirname(os.path.abspath('__file__'))\r\n",
        "save_file = dataset_dir + \"/mnist.pkl\"\r\n",
        "\r\n",
        "train_num = 60000\r\n",
        "test_num = 10000\r\n",
        "img_dim = (1, 28, 28)\r\n",
        "img_size = 784\r\n",
        "\r\n",
        "\r\n",
        "def _download(file_name):\r\n",
        "    file_path = dataset_dir + \"/\" + file_name\r\n",
        "    \r\n",
        "    if os.path.exists(file_path):\r\n",
        "        return\r\n",
        "\r\n",
        "    print(\"Downloading \" + file_name + \" ... \")\r\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\r\n",
        "    print(\"Done\")\r\n",
        "    \r\n",
        "def download_mnist():\r\n",
        "    for v in key_file.values():\r\n",
        "       _download(v)\r\n",
        "        \r\n",
        "def _load_label(file_name):\r\n",
        "    file_path = dataset_dir + \"/\" + file_name\r\n",
        "    \r\n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\r\n",
        "    with gzip.open(file_path, 'rb') as f:\r\n",
        "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\r\n",
        "    print(\"Done\")\r\n",
        "    \r\n",
        "    return labels\r\n",
        "\r\n",
        "def _load_img(file_name):\r\n",
        "    file_path = dataset_dir + \"/\" + file_name\r\n",
        "    \r\n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \r\n",
        "    with gzip.open(file_path, 'rb') as f:\r\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\r\n",
        "    data = data.reshape(-1, img_size)\r\n",
        "    print(\"Done\")\r\n",
        "    \r\n",
        "    return data\r\n",
        "    \r\n",
        "def _convert_numpy():\r\n",
        "    dataset = {}\r\n",
        "    dataset['train_img'] =  _load_img(key_file['train_img'])\r\n",
        "    dataset['train_label'] = _load_label(key_file['train_label'])    \r\n",
        "    dataset['test_img'] = _load_img(key_file['test_img'])\r\n",
        "    dataset['test_label'] = _load_label(key_file['test_label'])\r\n",
        "    \r\n",
        "    return dataset\r\n",
        "\r\n",
        "def init_mnist():\r\n",
        "    download_mnist()\r\n",
        "    dataset = _convert_numpy()\r\n",
        "    print(\"Creating pickle file ...\")\r\n",
        "    with open(save_file, 'wb') as f:\r\n",
        "        pickle.dump(dataset, f, -1)\r\n",
        "    print(\"Done!\")\r\n",
        "\r\n",
        "def _change_one_hot_label(X):\r\n",
        "    T = np.zeros((X.size, 10))\r\n",
        "    for idx, row in enumerate(T):\r\n",
        "        row[X[idx]] = 1\r\n",
        "        \r\n",
        "    return T\r\n",
        "    \r\n",
        "\r\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\r\n",
        "    \"\"\"MNIST 데이터셋 읽기\r\n",
        "    \r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\r\n",
        "    one_hot_label : \r\n",
        "        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\r\n",
        "        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\r\n",
        "    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \r\n",
        "    \r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\r\n",
        "    \"\"\"\r\n",
        "    if not os.path.exists(save_file):\r\n",
        "        init_mnist()\r\n",
        "        \r\n",
        "    with open(save_file, 'rb') as f:\r\n",
        "        dataset = pickle.load(f)\r\n",
        "    \r\n",
        "    if normalize:\r\n",
        "        for key in ('train_img', 'test_img'):\r\n",
        "            dataset[key] = dataset[key].astype(np.float32)\r\n",
        "            dataset[key] /= 255.0\r\n",
        "            \r\n",
        "    if one_hot_label:\r\n",
        "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\r\n",
        "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])    \r\n",
        "    \r\n",
        "    if not flatten:\r\n",
        "         for key in ('train_img', 'test_img'):\r\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\r\n",
        "\r\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    init_mnist()\r\n"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pfcM5lwIHvC"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2K40Ko4YqFG",
        "outputId": "7558609e-7e61-49db-d663-811f48534396"
      },
      "source": [
        "# 시그모이드 함수의 구현\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "  return 1/(1+np.exp(-x))\r\n",
        "\r\n",
        "x = np.array([-1.5, 2.5, 0])\r\n",
        "sigmoid(x)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.18242552, 0.92414182, 0.5       ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7pFjv6o_9v7"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)\r\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\r\n",
        "\r\n",
        "#hyperparameter\r\n",
        "iters_num = 10000\r\n",
        "train_size = x_train.shape[0]\r\n",
        "batch_size = 100\r\n",
        "learning_rate = 0.1\r\n",
        "\r\n",
        "train_loss_list = []\r\n",
        "train_acc_list = []\r\n",
        "test_acc_list = []\r\n",
        "\r\n",
        "# 1 에폭당 반복수\r\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\r\n",
        "\r\n",
        "for i in range(iters_num):\r\n",
        "  # mini batch 획득\r\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\r\n",
        "  x_batch = x_train[batch_mask]\r\n",
        "  t_batch = t_train[batch_mask]\r\n",
        "\r\n",
        "  # 기울기 계산\r\n",
        "  grad = network.gradient(x_batch, t_batch)\r\n",
        "\r\n",
        "  # 매개변수의 갱신\r\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\r\n",
        "    network.params[key] -= learning_rate * grad[key]\r\n",
        "  \r\n",
        "  # 학습경과의 기록\r\n",
        "  loss = network.loss(x_batch, t_batch)\r\n",
        "  train_loss_list.append(loss)\r\n",
        "\r\n",
        "  # 1에폭당의 정확도 계산\r\n",
        "  if i % iter_per_epoch == 0:\r\n",
        "    train_acc = network.accuracy(x_train, t_train)\r\n",
        "    test_acc = network.accuracy(x_test, t_test)\r\n",
        "    train_acc_list.append(train_acc)\r\n",
        "    test_acc_list.append(test_acc)\r\n",
        "    print(\"train acc, test acc |\" + str(train_acc) + \",\" + str(test_acc))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYMeChA0ECUs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}